import torch
import torchaudio
import numpy as np
import os
import random
from scipy.spatial.distance import cosine
from sklearn.metrics import roc_curve
import sounddevice as sd
from pynput import keyboard
sd.default.device = 0
torchaudio.set_audio_backend("soundfile")
# Load the ReDimNet model
def load_model():
    model = torch.hub.load('IDRnD/ReDimNet', 'b0', pretrained=True, finetuned=True)
    model.eval()
    return model


def record_audio_from_microphone(sample_rate=16000):
    """
    Records audio from the microphone indefinitely until Enter is pressed to stop.

    Args:
        sample_rate (int): The sampling rate of the audio.

    Returns:
        waveform (torch.Tensor): Recorded audio waveform tensor.
        sample_rate (int): The sample rate of the recording.
    """
    print("Press Enter to start recording.")
    input("Press Enter to start recording...")  # Wait for Enter to start recording
    print("Recording... Press Enter again to stop.")

    audio_chunks = []  # To store audio chunks
    stop_recording = False

    def on_press(key):
        nonlocal stop_recording
        if key == keyboard.Key.enter:
            print("Stopping recording...")
            stop_recording = True
            return False  # Stop listener

    # Set up a keyboard listener
    listener = keyboard.Listener(on_press=on_press)
    listener.start()

    try:
        # Open an input stream with the specified sample rate and mono channel
        with sd.InputStream(samplerate=sample_rate, channels=1, dtype='float32') as stream:
            while not stop_recording:
                # Read a small chunk of audio data
                chunk, _ = stream.read(1024)
                audio_chunks.append(chunk)

        # Concatenate all chunks into a single numpy array
        recording = np.concatenate(audio_chunks, axis=0)

        # Convert to PyTorch tensor and return
        waveform = torch.from_numpy(recording.T)
        return waveform, sample_rate

    except Exception as e:
        print(f"Error during recording: {e}")
        return None, None
    finally:
        listener.stop()

def inference_from_microphone(model):
    """
    Captures audio from the microphone with start/stop controlled by Enter key and processes it through the model.

    Args:
        model: The pre-trained model for inference.

    Returns:
        embedding (numpy.ndarray): The embedding generated by the model.
    """
    # Record audio from microphone
    waveform, sample_rate = record_audio_from_microphone(sample_rate=16000)
    
    if waveform is None or sample_rate is None:
        print("Recording failed.")
        return None
    
    # Ensure waveform is in the correct format (channels, samples)
    if waveform.dim() == 1:
        waveform = waveform.unsqueeze(0)  # Add channel dimension if missing
    
    # Generate embedding
    try:
        with torch.no_grad():
            embedding = model(waveform)
        
        # Flatten to ensure a 1-D array
        embedding = embedding.squeeze().cpu().numpy().flatten()
        return embedding
    except Exception as e:
        print(f"Error during inference: {e}")
        return None

# Process a single audio file into a spectrogram and then to an embedding
def process_audio(model, audio_path):
    waveform, sample_rate = torchaudio.load(audio_path)
    
    # Resample if necessary (e.g., to 16kHz)
    if sample_rate != 16000:
        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)
    
    # Generate embedding
    with torch.no_grad():
        embedding = model(waveform)  # Assuming model takes waveform input directly
    
    # Flatten to ensure a 1-D array and confirm shape
    embedding = embedding.squeeze().cpu().numpy().flatten()
    return embedding

# Generate reference embeddings from enrollment files
def generate_reference_embedding(model, enrollment_files):
    embeddings = []
    for file in enrollment_files:
        embedding = process_audio(model, file)
        embeddings.append(embedding)
    
    # Average embeddings to create a single reference embedding
    reference_embedding = np.mean(embeddings, axis=0)
    return reference_embedding

def evaluate_live(model, reference_embedding,test_embedding, speaker_id):
    thresh = .30
    if test_embedding is None:
        print("Inference failed.")
        return
    
    similarity = 1 - cosine(reference_embedding, test_embedding)
    if similarity >= thresh:
        print(f"This is {speaker_id}")
    else:
        print(f"This is not {speaker_id}")

def evaluate(model, reference_embedding, test_files, speaker_id):
    similarities = []
    labels = []
    
    for file in test_files:
        test_embedding = process_audio(model, file)
        similarity = 1 - cosine(reference_embedding, test_embedding)
        similarities.append(similarity)
        labels.append(1 if speaker_id in file else 0)  # Label as 1 if it belongs to speaker, else 0
    
    return np.array(similarities), np.array(labels)

# Calculate minDCF and EER from similarity scores and labels
def compute_metrics(similarities, labels, p_target=0.05, c_miss=1, c_fa=1, strictness="wst"):
    # Calculate False Acceptance Rate (FAR) and False Rejection Rate (FRR) at different thresholds
    fpr, tpr, thresholds = roc_curve(labels, similarities)
    fnr = 1 - tpr
    
    # Find EER thresholds
    eer_indices = np.where(np.abs(fnr - fpr) == np.min(np.abs(fnr - fpr)))[0]
    eer = fpr[eer_indices[0]]
    
    if strictness == "more":
        eer_threshold = thresholds[eer_indices[-1]]  # More strict (higher threshold)
    elif strictness == "less":
        eer_threshold = thresholds[eer_indices[0]]  # Less strict (lower threshold)
    else:
        eer_threshold = np.mean(thresholds[eer_indices])  # Average of all EER thresholds

    # Calculate minDCF
    c_det = c_miss * fnr * p_target + c_fa * fpr * (1 - p_target)
    min_dcf_indices = np.where(c_det == np.min(c_det))[0]
    min_dcf = c_det[min_dcf_indices[0]]
    
    if strictness == "more":
        min_dcf_threshold = thresholds[min_dcf_indices[-1]]  # More strict (higher threshold)
    elif strictness == "less":
        min_dcf_threshold = thresholds[min_dcf_indices[0]]  # Less strict (lower threshold)
    else:
        min_dcf_threshold = np.mean(thresholds[min_dcf_indices])  # Average of all minDCF thresholds
    
    return eer, min_dcf, eer_threshold, min_dcf_threshold

def load_test_files(file_path, base_dir):
    """Load all test files from testset.txt."""
    with open(file_path, 'r') as f:
        files = [os.path.join(base_dir, line.strip()) for line in f]
    return files

def sample_enrollment_files(test_files, speaker, num_samples=3):
    """Randomly sample files for enrollment for the specified speaker without replacement and update test files."""
    # Filter files by speaker
    speaker_files = [f for f in test_files if speaker in os.path.basename(f)]
    
    # Sample 3-5 files for enrollment
    enrollment_files = random.sample(speaker_files, num_samples)
    
    # Remove the selected enrollment files from test_files
    test_files = [f for f in test_files if f not in enrollment_files]
    
    return enrollment_files, test_files

def experiment_for_each_speaker(model, testset_file, base_dir, log_file="logs.txt"):
    test_files = load_test_files(testset_file, base_dir)
    speakers = set(os.path.basename(os.path.dirname(file)) for file in test_files)
    results = {}

    with open(log_file, "w") as log:
        for speaker in speakers:
            print(f"\nRunning experiment for speaker: {speaker}")
            log.write(f"Speaker: {speaker}\n")
            
            # Sample enrollment files for the current speaker
            speaker_files = [f for f in test_files if os.path.basename(os.path.dirname(f)) == speaker]
            if len(speaker_files) < 3:
                print(f"Skipping {speaker}: Not enough samples for enrollment and testing")
                log.write("Skipping: Not enough samples for enrollment and testing\n")
                continue
            
            num_samples = random.randint(3, min(5, len(speaker_files)))
            enrollment_files = random.sample(speaker_files, num_samples)
            remaining_files = [f for f in test_files if f not in enrollment_files]
            
            # Log the enrollment files
            log.write("Enrollment files:\n")
            for ef in enrollment_files:
                log.write(f"  {ef}\n")
            
            # Generate reference embedding
            reference_embedding = generate_reference_embedding(model, enrollment_files)
            
            # Evaluate on remaining test files
            similarities, labels = evaluate(model, reference_embedding, remaining_files, speaker)
            
            # Compute EER and minDCF
            eer, min_dcf, T1, T2 = compute_metrics(similarities, labels)
            results[speaker] = {"EER": eer, "minDCF": min_dcf, "EER_T": T1, "minDCF_T": T2}

            print(f"Speaker: {speaker} - EER: {eer * 100:.4f}%, minDCF: {min_dcf:.4f}")
            print(f"Thresholds - EER: {T1:.4f}%, minDCF: {T2:.4f}")
            log.write(f"EER: {eer * 100:.2f}%, minDCF: {min_dcf:.4f}%,Thresholds - EER: {T1:.4f}%, minDCF: {T2:.4f}\n\n")
    
    return results


def experiment_for_each_speaker_live(model, testset_file, base_dir, log_file="logs.txt"):
    test_files = load_test_files(testset_file, base_dir)
    speakers = set(os.path.basename(os.path.dirname(file)) for file in test_files)
    test_embedding = inference_from_microphone(model)
    with open(log_file, "w") as log:
        for speaker in speakers:
            print(f"\nRunning experiment for speaker: {speaker}")
            log.write(f"Speaker: {speaker}\n")
            
            # Sample enrollment files for the current speaker
            speaker_files = [f for f in test_files if os.path.basename(os.path.dirname(f)) == speaker]
            if len(speaker_files) < 4:
                print(f"Skipping {speaker}: Not enough samples for enrollment and testing")
                log.write("Skipping: Not enough samples for enrollment and testing\n")
                continue
            
            num_samples = random.randint(4, min(6, len(speaker_files)))
            enrollment_files = random.sample(speaker_files, num_samples)
            
            # Log the enrollment files
            log.write("Enrollment files:\n")
            for ef in enrollment_files:
                log.write(f"  {ef}\n")
            
            # Generate reference embedding
            reference_embedding = generate_reference_embedding(model, enrollment_files)
            
            # Evaluate on remaining test files
            evaluate_live(model, reference_embedding, test_embedding, speaker)

# Example usage
if __name__ == "__main__":
    model = load_model()
    base_folder = "/Users/abdulrahmanbanabila/Documents/TII/codebases/ReDim"
    testset_file = os.path.join(base_folder, "testset.txt")
    live = False
    if live :
        experiment_for_each_speaker_live(model, testset_file, base_folder)
        exit()
    # Run experiment for each speaker and collect results
    all_results = experiment_for_each_speaker(model, testset_file, base_folder)
    
    # Display final summary
    print("\nFinal Results Summary:")
    for speaker, metrics in all_results.items():
        print(f"Speaker: {speaker}, EER: {metrics['EER'] * 100:.2f}%, minDCF: {metrics['minDCF']:.4f}")